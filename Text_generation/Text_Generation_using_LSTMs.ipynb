{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/H0sseinR0stami/DeepLearningProjects/blob/main/Text_generation/Text_Generation_using_LSTMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZkt1yHW1Gm5",
        "outputId": "d56d3eda-a45b-4dae-a6ce-5d58af58387a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl6BjdGkF2BK"
      },
      "source": [
        "\n",
        "\n",
        "**1. Import the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu5UjPJTW9GW"
      },
      "source": [
        "\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "import keras\n",
        "from numpy.random import seed\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n",
        "seed(1)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, os\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge9PgTIeF6rI"
      },
      "source": [
        "**2. Load the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gypm2_zVcm-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "371683bc-a97a-43b6-9a45-ba42b78d9f16"
      },
      "source": [
        "curr_dir = '/content/drive/MyDrive/Colab_Notebooks/project5/database/'\n",
        "all_headlines = []\n",
        "for filename in os.listdir(curr_dir):\n",
        "    if 'Articles' in filename:\n",
        "       print(filename)\n",
        "       article_df = pd.read_csv(curr_dir + filename)\n",
        "       all_headlines.extend(list(article_df.headline.values))\n",
        "    #if 'CommentsJan2017' in filename:\n",
        "       #print(filename)\n",
        "       #article_df = pd.read_csv(curr_dir + filename,nrows=500)\n",
        "       #all_headlines.extend(list(article_df.commentBody.values))\n",
        "       ##df = pd.read_csv('matrix.txt', sep=',', header=None, skiprows=1000, nrows=1000)\n",
        "all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
        "len(all_headlines)\n",
        "\n",
        "\n",
        "\n",
        "all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
        "len(all_headlines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ArticlesApril2017.csv\n",
            "ArticlesFeb2017.csv\n",
            "ArticlesFeb2018.csv\n",
            "ArticlesApril2018.csv\n",
            "ArticlesJan2017.csv\n",
            "ArticlesJan2018.csv\n",
            "ArticlesMarch2017.csv\n",
            "ArticlesMarch2018.csv\n",
            "ArticlesMay2017.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8603"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Dataset preparation**"
      ],
      "metadata": {
        "id": "qFJwK656IMNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1 Dataset cleaning\n",
        "\n",
        "In dataset preparation step, we will first perform text cleaning of the data which includes removal of punctuations and lower casing all the words."
      ],
      "metadata": {
        "id": "vZRV9cR1IRxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(txt):\n",
        "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
        "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
        "    return txt\n",
        "\n",
        "corpus = [clean_text(x) for x in all_headlines]\n",
        "corpus[:10]\n"
      ],
      "metadata": {
        "id": "5tcao-C_6cK0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8aabe2f-9845-45ee-b7ed-a7a5dff29e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['finding an expansive view  of a forgotten people in niger',\n",
              " 'and now  the dreaded trump curse',\n",
              " 'venezuelas descent into dictatorship',\n",
              " 'stain permeates basketball blue blood',\n",
              " 'taking things for granted',\n",
              " 'the caged beast awakens',\n",
              " 'an everunfolding story',\n",
              " 'oreilly thrives as settlements add up',\n",
              " 'mouse infestation',\n",
              " 'divide in gop now threatens trump tax plan']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2 Generating Sequence of N-gram Tokens**\n",
        "\n",
        "Language modelling requires a sequence input data, as given a sequence (of words/tokens) the aim is the predict next word/token.\n",
        "\n",
        "The next step is Tokenization. Tokenization is a process of extracting tokens (terms / words) from a corpus. Pythonâ€™s library Keras has inbuilt model for tokenization which can be used to obtain the tokens and their index in the corpus. After this step, every text document in the dataset is converted into sequence of tokens."
      ],
      "metadata": {
        "id": "VuVhZD_2Icao"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhasEsGmWc15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03d02e5f-9bce-495f-f42d-d14e619b32a9"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "def get_sequence_of_tokens(corpus):\n",
        "    ## tokenization\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "    ## convert data to sequence of tokens\n",
        "    input_sequences = []\n",
        "    for line in corpus:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "    return input_sequences, total_words\n",
        "\n",
        "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
        "inp_sequences[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[391, 17],\n",
              " [391, 17, 5166],\n",
              " [391, 17, 5166, 523],\n",
              " [391, 17, 5166, 523, 4],\n",
              " [391, 17, 5166, 523, 4, 2],\n",
              " [391, 17, 5166, 523, 4, 2, 1601],\n",
              " [391, 17, 5166, 523, 4, 2, 1601, 134],\n",
              " [391, 17, 5166, 523, 4, 2, 1601, 134, 5],\n",
              " [391, 17, 5166, 523, 4, 2, 1601, 134, 5, 1951],\n",
              " [7, 57]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUi4h9WKGN_q"
      },
      "source": [
        "**3.3 Padding the Sequences and obtain Variables : Predictors and Target**\n",
        "\n",
        "Now that we have generated a data-set which contains sequence of tokens, it is possible that different sequences have different lengths. Before starting training the model, we need to pad the sequences and make their lengths equal. We can use pad_sequence function of Kears for this purpose. To input this data into a learning model, we need to create predictors and label. We will create N-grams sequence as predictors and the next word of the N-gram as label. For example:\n",
        "\n",
        "Headline: they are learning data science\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBz4MO3NXVPW"
      },
      "source": [
        "def generate_padded_sequences(input_sequences):\n",
        "    max_sequence_len = max([len(x) for x in input_sequences])\n",
        "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "    label = keras.utils.np_utils.to_categorical(label, num_classes=total_words)\n",
        "    return predictors, label, max_sequence_len\n",
        "\n",
        "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJM0X-gQHLig"
      },
      "source": [
        "**4. LSTMs for Text Generation**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike Feed-forward neural networks in which activation outputs are propagated only in one direction, the activation outputs from neurons propagate in both directions (from inputs to outputs and from outputs to inputs) in Recurrent Neural Networks. This creates loops in the neural network architecture which acts as a â€˜memory stateâ€™ of the neurons. This state allows the neurons an ability to remember what have been learned so far.\n",
        "\n",
        "The memory state in RNNs gives an advantage over traditional neural networks but a problem called Vanishing Gradient is associated with them. In this problem, while learning with a large number of layers, it becomes really hard for the network to learn and tune the parameters of the earlier layers. To address this problem, A new type of RNNs called LSTMs (Long Short Term Memory) Models have been developed.\n",
        "\n",
        "LSTMs have an additional state called â€˜cell stateâ€™ through which the network makes adjustments in the information flow. The advantage of this state is that the model can remember or forget the leanings more selectively. To learn more about LSTMs, here is a great post. Lets architecture a LSTM model in our code. I have added total three layers in the model.\n",
        "\n",
        "Input Layer : Takes the sequence of words as input\n",
        "LSTM Layer : Computes the output using LSTM units. I have added 100 units in the layer, but this number can be fine tuned later.\n",
        "Dropout Layer : A regularisation layer which randomly turns-off the activations of some neurons in the LSTM layer. It helps in preventing over fitting. (Optional Layer)\n",
        "Output Layer : Computes the probability of the best possible next word as output\n",
        "We will run this model for total 100 epoochs but it can be experimented further."
      ],
      "metadata": {
        "id": "GHqWjXXiJs8k"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1_iKB2AWonF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bc069a4-0517-458e-818a-6d6a7cd24bd0"
      },
      "source": [
        "def create_model(max_sequence_len, total_words):\n",
        "    input_len = max_sequence_len - 1\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add Input Embedding Layer\n",
        "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
        "\n",
        "    # Add Hidden Layer 1 - LSTM Layer\n",
        "    model.add(LSTM(200))\n",
        "    model.add(Dropout(0.15))\n",
        "\n",
        "    # Add Output Layer\n",
        "    model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_model(max_sequence_len, total_words)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 23, 10)            112650    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 200)               168800    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 200)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 11265)             2264265   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,545,715\n",
            "Trainable params: 2,545,715\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpointing\n",
        "checkpoint_path = \"/content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\"\n",
        "\n",
        "\n",
        "# Create a callback that saves the best model\n",
        "mc = tensorflow.keras.callbacks.ModelCheckpoint(\n",
        "    monitor=\"loss\",\n",
        "    filepath=checkpoint_path,\n",
        "    verbose=1,\n",
        "    save_best_only= True , mode='auto'\n",
        ")\n",
        "\n",
        "# early stopping\n",
        "\n",
        "#es = keras.callbacks.EarlyStopping(monitor = \"loss\" , min_delta = 0.01 , patience = 10 , verbose =1 , mode = 'auto' )\n",
        "\n",
        "# model check point\n",
        "\n",
        "#cp_callback = [es,mc]\n"
      ],
      "metadata": {
        "id": "ThBPiRD_RKrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9AV1cUQHSLg"
      },
      "source": [
        "**Lets train our model now**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R3aBtYFr_5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12df1d53-5c98-48e6-9d95-9fa4002aa8f7"
      },
      "source": [
        "model.fit(predictors, label, epochs=500,steps_per_epoch=1500, verbose=1, callbacks=[mc])\n",
        "#model.fit(predictors, label, epochs=500, verbose=1, callbacks=[cp_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 6.7634\n",
            "Epoch 00001: loss improved from 7.23863 to 6.76336, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 6.7634\n",
            "Epoch 2/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 6.8313\n",
            "Epoch 00002: loss did not improve from 6.76336\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 6.8313\n",
            "Epoch 3/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 6.6460\n",
            "Epoch 00003: loss improved from 6.76336 to 6.64537, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 6.6454\n",
            "Epoch 4/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 6.3840\n",
            "Epoch 00004: loss improved from 6.64537 to 6.38256, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 6.3826\n",
            "Epoch 5/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 6.0816\n",
            "Epoch 00005: loss improved from 6.38256 to 6.08102, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 6.0810\n",
            "Epoch 6/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 5.7412\n",
            "Epoch 00006: loss improved from 6.08102 to 5.74010, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 5.7401\n",
            "Epoch 7/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 5.4007\n",
            "Epoch 00007: loss improved from 5.74010 to 5.39986, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 5.3999\n",
            "Epoch 8/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 5.0736\n",
            "Epoch 00008: loss improved from 5.39986 to 5.07235, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 5.0723\n",
            "Epoch 9/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 4.7692\n",
            "Epoch 00009: loss improved from 5.07235 to 4.76917, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 4.7692\n",
            "Epoch 10/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 4.4877\n",
            "Epoch 00010: loss improved from 4.76917 to 4.48725, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 4.4873\n",
            "Epoch 11/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 4.2257\n",
            "Epoch 00011: loss improved from 4.48725 to 4.22450, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 4.2245\n",
            "Epoch 12/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 3.9879\n",
            "Epoch 00012: loss improved from 4.22450 to 3.98790, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 3.9879\n",
            "Epoch 13/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 3.7835\n",
            "Epoch 00013: loss improved from 3.98790 to 3.78155, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 3.7816\n",
            "Epoch 14/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 3.5954\n",
            "Epoch 00014: loss improved from 3.78155 to 3.59629, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 3.5963\n",
            "Epoch 15/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 3.4233\n",
            "Epoch 00015: loss improved from 3.59629 to 3.42205, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 3.4220\n",
            "Epoch 16/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 3.2609\n",
            "Epoch 00016: loss improved from 3.42205 to 3.26035, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 3.2603\n",
            "Epoch 17/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 3.1229\n",
            "Epoch 00017: loss improved from 3.26035 to 3.12210, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 3.1221\n",
            "Epoch 18/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 2.9846\n",
            "Epoch 00018: loss improved from 3.12210 to 2.98461, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 2.9846\n",
            "Epoch 19/500\n",
            "1492/1500 [============================>.] - ETA: 0s - loss: 2.8568\n",
            "Epoch 00019: loss improved from 2.98461 to 2.85561, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 2.8556\n",
            "Epoch 20/500\n",
            "1492/1500 [============================>.] - ETA: 0s - loss: 2.7530\n",
            "Epoch 00020: loss improved from 2.85561 to 2.75313, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 2.7531\n",
            "Epoch 21/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 2.6566\n",
            "Epoch 00021: loss improved from 2.75313 to 2.65614, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 2.6561\n",
            "Epoch 22/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 2.5462\n",
            "Epoch 00022: loss improved from 2.65614 to 2.54547, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 2.5455\n",
            "Epoch 23/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 2.4705\n",
            "Epoch 00023: loss improved from 2.54547 to 2.46995, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 2.4699\n",
            "Epoch 24/500\n",
            "1492/1500 [============================>.] - ETA: 0s - loss: 2.3810\n",
            "Epoch 00024: loss improved from 2.46995 to 2.37976, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 2.3798\n",
            "Epoch 25/500\n",
            "1492/1500 [============================>.] - ETA: 0s - loss: 2.3019\n",
            "Epoch 00025: loss improved from 2.37976 to 2.29945, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 2.2994\n",
            "Epoch 26/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 2.2200\n",
            "Epoch 00026: loss improved from 2.29945 to 2.21983, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 2.2198\n",
            "Epoch 27/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 2.1769\n",
            "Epoch 00027: loss improved from 2.21983 to 2.17663, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 2.1766\n",
            "Epoch 28/500\n",
            "1492/1500 [============================>.] - ETA: 0s - loss: 2.1064\n",
            "Epoch 00028: loss improved from 2.17663 to 2.10720, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 2.1072\n",
            "Epoch 29/500\n",
            "1492/1500 [============================>.] - ETA: 0s - loss: 2.0445\n",
            "Epoch 00029: loss improved from 2.10720 to 2.04332, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 2.0433\n",
            "Epoch 30/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.9927\n",
            "Epoch 00030: loss improved from 2.04332 to 1.99268, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.9927\n",
            "Epoch 31/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.9369\n",
            "Epoch 00031: loss improved from 1.99268 to 1.93631, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.9363\n",
            "Epoch 32/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.8937\n",
            "Epoch 00032: loss improved from 1.93631 to 1.89349, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.8935\n",
            "Epoch 33/500\n",
            "1492/1500 [============================>.] - ETA: 0s - loss: 1.8523\n",
            "Epoch 00033: loss improved from 1.89349 to 1.85217, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.8522\n",
            "Epoch 34/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.8054\n",
            "Epoch 00034: loss improved from 1.85217 to 1.80514, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.8051\n",
            "Epoch 35/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.7740\n",
            "Epoch 00035: loss improved from 1.80514 to 1.77293, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.7729\n",
            "Epoch 36/500\n",
            "1492/1500 [============================>.] - ETA: 0s - loss: 1.7174\n",
            "Epoch 00036: loss improved from 1.77293 to 1.71756, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.7176\n",
            "Epoch 37/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.6895\n",
            "Epoch 00037: loss improved from 1.71756 to 1.68938, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.6894\n",
            "Epoch 38/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.6676\n",
            "Epoch 00038: loss improved from 1.68938 to 1.66716, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.6672\n",
            "Epoch 39/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.6267\n",
            "Epoch 00039: loss improved from 1.66716 to 1.62700, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.6270\n",
            "Epoch 40/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.5890\n",
            "Epoch 00040: loss improved from 1.62700 to 1.58920, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.5892\n",
            "Epoch 41/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.5683\n",
            "Epoch 00041: loss improved from 1.58920 to 1.56897, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.5690\n",
            "Epoch 42/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.5470\n",
            "Epoch 00042: loss improved from 1.56897 to 1.54730, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.5473\n",
            "Epoch 43/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.5303\n",
            "Epoch 00043: loss improved from 1.54730 to 1.52962, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.5296\n",
            "Epoch 44/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.4978\n",
            "Epoch 00044: loss improved from 1.52962 to 1.49789, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.4979\n",
            "Epoch 45/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.4763\n",
            "Epoch 00045: loss improved from 1.49789 to 1.47628, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.4763\n",
            "Epoch 46/500\n",
            "1492/1500 [============================>.] - ETA: 0s - loss: 1.4558\n",
            "Epoch 00046: loss improved from 1.47628 to 1.45547, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.4555\n",
            "Epoch 47/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.4350\n",
            "Epoch 00047: loss improved from 1.45547 to 1.43498, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.4350\n",
            "Epoch 48/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.4159\n",
            "Epoch 00048: loss improved from 1.43498 to 1.41606, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.4161\n",
            "Epoch 49/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.4036\n",
            "Epoch 00049: loss improved from 1.41606 to 1.40412, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.4041\n",
            "Epoch 50/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.4101\n",
            "Epoch 00050: loss did not improve from 1.40412\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.4104\n",
            "Epoch 51/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.3652\n",
            "Epoch 00051: loss improved from 1.40412 to 1.36556, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.3656\n",
            "Epoch 52/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.3439\n",
            "Epoch 00052: loss improved from 1.36556 to 1.34406, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.3441\n",
            "Epoch 53/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.3404\n",
            "Epoch 00053: loss improved from 1.34406 to 1.34092, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.3409\n",
            "Epoch 54/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.3430\n",
            "Epoch 00054: loss did not improve from 1.34092\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.3432\n",
            "Epoch 55/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.3232\n",
            "Epoch 00055: loss improved from 1.34092 to 1.32471, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.3247\n",
            "Epoch 56/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.3033\n",
            "Epoch 00056: loss improved from 1.32471 to 1.30427, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.3043\n",
            "Epoch 57/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.3005\n",
            "Epoch 00057: loss improved from 1.30427 to 1.30012, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.3001\n",
            "Epoch 58/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.2759\n",
            "Epoch 00058: loss improved from 1.30012 to 1.27598, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.2760\n",
            "Epoch 59/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.2675\n",
            "Epoch 00059: loss improved from 1.27598 to 1.26912, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.2691\n",
            "Epoch 60/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.2695\n",
            "Epoch 00060: loss did not improve from 1.26912\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.2705\n",
            "Epoch 61/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.2632\n",
            "Epoch 00061: loss improved from 1.26912 to 1.26320, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.2632\n",
            "Epoch 62/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.2405\n",
            "Epoch 00062: loss improved from 1.26320 to 1.24006, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.2401\n",
            "Epoch 63/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.2499\n",
            "Epoch 00063: loss did not improve from 1.24006\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.2494\n",
            "Epoch 64/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.2466\n",
            "Epoch 00064: loss did not improve from 1.24006\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.2466\n",
            "Epoch 65/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.2291\n",
            "Epoch 00065: loss improved from 1.24006 to 1.22926, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.2293\n",
            "Epoch 66/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.2090\n",
            "Epoch 00066: loss improved from 1.22926 to 1.20975, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.2098\n",
            "Epoch 67/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.2074\n",
            "Epoch 00067: loss improved from 1.20975 to 1.20874, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.2087\n",
            "Epoch 68/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.2056\n",
            "Epoch 00068: loss improved from 1.20874 to 1.20598, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.2060\n",
            "Epoch 69/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.2011\n",
            "Epoch 00069: loss improved from 1.20598 to 1.20131, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.2013\n",
            "Epoch 70/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.2101\n",
            "Epoch 00070: loss did not improve from 1.20131\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.2102\n",
            "Epoch 71/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.1795\n",
            "Epoch 00071: loss improved from 1.20131 to 1.17919, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1792\n",
            "Epoch 72/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.1716\n",
            "Epoch 00072: loss improved from 1.17919 to 1.17188, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.1719\n",
            "Epoch 73/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.1757\n",
            "Epoch 00073: loss did not improve from 1.17188\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.1763\n",
            "Epoch 74/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.1638\n",
            "Epoch 00074: loss improved from 1.17188 to 1.16410, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1641\n",
            "Epoch 75/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.1602\n",
            "Epoch 00075: loss improved from 1.16410 to 1.16040, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.1604\n",
            "Epoch 76/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.1588\n",
            "Epoch 00076: loss improved from 1.16040 to 1.15882, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1588\n",
            "Epoch 77/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.1672\n",
            "Epoch 00077: loss did not improve from 1.15882\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.1671\n",
            "Epoch 78/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.1416\n",
            "Epoch 00078: loss improved from 1.15882 to 1.14161, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1416\n",
            "Epoch 79/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.1490\n",
            "Epoch 00079: loss did not improve from 1.14161\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.1484\n",
            "Epoch 80/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.1258\n",
            "Epoch 00080: loss improved from 1.14161 to 1.12577, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1258\n",
            "Epoch 81/500\n",
            "1492/1500 [============================>.] - ETA: 0s - loss: 1.1208\n",
            "Epoch 00081: loss improved from 1.12577 to 1.12052, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.1205\n",
            "Epoch 82/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.8553\n",
            "Epoch 00082: loss did not improve from 1.12052\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.8525\n",
            "Epoch 83/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.3970\n",
            "Epoch 00083: loss did not improve from 1.12052\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.3958\n",
            "Epoch 84/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.2445\n",
            "Epoch 00084: loss did not improve from 1.12052\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.2445\n",
            "Epoch 85/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.2169\n",
            "Epoch 00085: loss did not improve from 1.12052\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.2173\n",
            "Epoch 86/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.1837\n",
            "Epoch 00086: loss did not improve from 1.12052\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.1837\n",
            "Epoch 87/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.1792\n",
            "Epoch 00087: loss did not improve from 1.12052\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.1788\n",
            "Epoch 88/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.1666\n",
            "Epoch 00088: loss did not improve from 1.12052\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.1666\n",
            "Epoch 89/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.1442\n",
            "Epoch 00089: loss did not improve from 1.12052\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.1448\n",
            "Epoch 90/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.1391\n",
            "Epoch 00090: loss did not improve from 1.12052\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.1384\n",
            "Epoch 91/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.1257\n",
            "Epoch 00091: loss did not improve from 1.12052\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.1257\n",
            "Epoch 92/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.1131\n",
            "Epoch 00092: loss improved from 1.12052 to 1.11313, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1131\n",
            "Epoch 93/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.1284\n",
            "Epoch 00093: loss did not improve from 1.11313\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.1281\n",
            "Epoch 94/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.1176\n",
            "Epoch 00094: loss did not improve from 1.11313\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.1174\n",
            "Epoch 95/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.1024\n",
            "Epoch 00095: loss improved from 1.11313 to 1.10152, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1015\n",
            "Epoch 96/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.1041\n",
            "Epoch 00096: loss did not improve from 1.10152\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.1041\n",
            "Epoch 97/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0906\n",
            "Epoch 00097: loss improved from 1.10152 to 1.09060, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0906\n",
            "Epoch 98/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.1009\n",
            "Epoch 00098: loss did not improve from 1.09060\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.1013\n",
            "Epoch 99/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0796\n",
            "Epoch 00099: loss improved from 1.09060 to 1.07973, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0797\n",
            "Epoch 100/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0683\n",
            "Epoch 00100: loss improved from 1.07973 to 1.06833, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.0683\n",
            "Epoch 101/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.0812\n",
            "Epoch 00101: loss did not improve from 1.06833\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0812\n",
            "Epoch 102/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.0673\n",
            "Epoch 00102: loss improved from 1.06833 to 1.06723, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0672\n",
            "Epoch 103/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.0966\n",
            "Epoch 00103: loss did not improve from 1.06723\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0969\n",
            "Epoch 104/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.0779\n",
            "Epoch 00104: loss did not improve from 1.06723\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0780\n",
            "Epoch 105/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.0673\n",
            "Epoch 00105: loss did not improve from 1.06723\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0675\n",
            "Epoch 106/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.0657\n",
            "Epoch 00106: loss improved from 1.06723 to 1.06593, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0659\n",
            "Epoch 107/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0564\n",
            "Epoch 00107: loss improved from 1.06593 to 1.05639, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0564\n",
            "Epoch 108/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.1733\n",
            "Epoch 00108: loss did not improve from 1.05639\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.1735\n",
            "Epoch 109/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.0869\n",
            "Epoch 00109: loss did not improve from 1.05639\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0867\n",
            "Epoch 110/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.0800\n",
            "Epoch 00110: loss did not improve from 1.05639\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0797\n",
            "Epoch 111/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.1869\n",
            "Epoch 00111: loss did not improve from 1.05639\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.1862\n",
            "Epoch 112/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0678\n",
            "Epoch 00112: loss did not improve from 1.05639\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0678\n",
            "Epoch 113/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0543\n",
            "Epoch 00113: loss improved from 1.05639 to 1.05404, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0540\n",
            "Epoch 114/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0299\n",
            "Epoch 00114: loss improved from 1.05404 to 1.02957, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.0296\n",
            "Epoch 115/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0127\n",
            "Epoch 00115: loss improved from 1.02957 to 1.01271, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0127\n",
            "Epoch 116/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0136\n",
            "Epoch 00116: loss did not improve from 1.01271\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0135\n",
            "Epoch 117/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0226\n",
            "Epoch 00117: loss did not improve from 1.01271\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0222\n",
            "Epoch 118/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0387\n",
            "Epoch 00118: loss did not improve from 1.01271\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0385\n",
            "Epoch 119/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.0519\n",
            "Epoch 00119: loss did not improve from 1.01271\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0515\n",
            "Epoch 120/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0316\n",
            "Epoch 00120: loss did not improve from 1.01271\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0329\n",
            "Epoch 121/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0342\n",
            "Epoch 00121: loss did not improve from 1.01271\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0342\n",
            "Epoch 122/500\n",
            "1492/1500 [============================>.] - ETA: 0s - loss: 1.0301\n",
            "Epoch 00122: loss did not improve from 1.01271\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0296\n",
            "Epoch 123/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0341\n",
            "Epoch 00123: loss did not improve from 1.01271\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0346\n",
            "Epoch 124/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0100\n",
            "Epoch 00124: loss improved from 1.01271 to 1.01017, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0102\n",
            "Epoch 125/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0191\n",
            "Epoch 00125: loss did not improve from 1.01017\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0188\n",
            "Epoch 126/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0141\n",
            "Epoch 00126: loss did not improve from 1.01017\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0138\n",
            "Epoch 127/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.0006\n",
            "Epoch 00127: loss improved from 1.01017 to 1.00102, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0010\n",
            "Epoch 128/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.0201\n",
            "Epoch 00128: loss did not improve from 1.00102\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0204\n",
            "Epoch 129/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0223\n",
            "Epoch 00129: loss did not improve from 1.00102\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0225\n",
            "Epoch 130/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0179\n",
            "Epoch 00130: loss did not improve from 1.00102\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0185\n",
            "Epoch 131/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0353\n",
            "Epoch 00131: loss did not improve from 1.00102\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0354\n",
            "Epoch 132/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.0602\n",
            "Epoch 00132: loss did not improve from 1.00102\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0612\n",
            "Epoch 133/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0882\n",
            "Epoch 00133: loss did not improve from 1.00102\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0882\n",
            "Epoch 134/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0308\n",
            "Epoch 00134: loss did not improve from 1.00102\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0308\n",
            "Epoch 135/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0301\n",
            "Epoch 00135: loss did not improve from 1.00102\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0302\n",
            "Epoch 136/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0233\n",
            "Epoch 00136: loss did not improve from 1.00102\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0233\n",
            "Epoch 137/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.0273\n",
            "Epoch 00137: loss did not improve from 1.00102\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0277\n",
            "Epoch 138/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0344\n",
            "Epoch 00138: loss did not improve from 1.00102\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0344\n",
            "Epoch 139/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.0191\n",
            "Epoch 00139: loss did not improve from 1.00102\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0191\n",
            "Epoch 140/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.0087\n",
            "Epoch 00140: loss did not improve from 1.00102\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0093\n",
            "Epoch 141/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 0.9920\n",
            "Epoch 00141: loss improved from 1.00102 to 0.99306, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 0.9931\n",
            "Epoch 142/500\n",
            "1492/1500 [============================>.] - ETA: 0s - loss: 1.0043\n",
            "Epoch 00142: loss did not improve from 0.99306\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.0052\n",
            "Epoch 143/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0011\n",
            "Epoch 00143: loss did not improve from 0.99306\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 1.0016\n",
            "Epoch 144/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0036\n",
            "Epoch 00144: loss did not improve from 0.99306\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.0038\n",
            "Epoch 145/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 0.9915\n",
            "Epoch 00145: loss improved from 0.99306 to 0.99195, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 0.9919\n",
            "Epoch 146/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0205\n",
            "Epoch 00146: loss did not improve from 0.99195\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.0205\n",
            "Epoch 147/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 0.9998\n",
            "Epoch 00147: loss did not improve from 0.99195\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0002\n",
            "Epoch 148/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.0026\n",
            "Epoch 00148: loss did not improve from 0.99195\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0029\n",
            "Epoch 149/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 0.9997\n",
            "Epoch 00149: loss did not improve from 0.99195\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 0.9994\n",
            "Epoch 150/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.0132\n",
            "Epoch 00150: loss did not improve from 0.99195\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0133\n",
            "Epoch 151/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0031\n",
            "Epoch 00151: loss did not improve from 0.99195\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.0028\n",
            "Epoch 152/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0093\n",
            "Epoch 00152: loss did not improve from 0.99195\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.0096\n",
            "Epoch 153/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 0.9832\n",
            "Epoch 00153: loss improved from 0.99195 to 0.98256, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 0.9826\n",
            "Epoch 154/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0008\n",
            "Epoch 00154: loss did not improve from 0.98256\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.0009\n",
            "Epoch 155/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.0000\n",
            "Epoch 00155: loss did not improve from 0.98256\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0001\n",
            "Epoch 156/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.0250\n",
            "Epoch 00156: loss did not improve from 0.98256\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.0254\n",
            "Epoch 157/500\n",
            "1492/1500 [============================>.] - ETA: 0s - loss: 1.0571\n",
            "Epoch 00157: loss did not improve from 0.98256\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.0572\n",
            "Epoch 158/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 0.9708\n",
            "Epoch 00158: loss improved from 0.98256 to 0.97047, saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/bestmodel.h5\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 0.9705\n",
            "Epoch 159/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 0.9715\n",
            "Epoch 00159: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 0.9716\n",
            "Epoch 160/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 0.9879\n",
            "Epoch 00160: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 0.9879\n",
            "Epoch 161/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 0.9969\n",
            "Epoch 00161: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 0.9969\n",
            "Epoch 162/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0028\n",
            "Epoch 00162: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.0022\n",
            "Epoch 163/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 0.9862\n",
            "Epoch 00163: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 0.9858\n",
            "Epoch 164/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0048\n",
            "Epoch 00164: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0048\n",
            "Epoch 165/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 0.9995\n",
            "Epoch 00165: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 0.9995\n",
            "Epoch 166/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.0131\n",
            "Epoch 00166: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0127\n",
            "Epoch 167/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 0.9819\n",
            "Epoch 00167: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 0.9819\n",
            "Epoch 168/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 0.9906\n",
            "Epoch 00168: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 0.9906\n",
            "Epoch 169/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 0.9921\n",
            "Epoch 00169: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 0.9917\n",
            "Epoch 170/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 0.9993\n",
            "Epoch 00170: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 0.9994\n",
            "Epoch 171/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.0105\n",
            "Epoch 00171: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.0097\n",
            "Epoch 172/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0226\n",
            "Epoch 00172: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0221\n",
            "Epoch 173/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 0.9906\n",
            "Epoch 00173: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 0.9912\n",
            "Epoch 174/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.0033\n",
            "Epoch 00174: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0032\n",
            "Epoch 175/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0367\n",
            "Epoch 00175: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0367\n",
            "Epoch 176/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0791\n",
            "Epoch 00176: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0792\n",
            "Epoch 177/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.0573\n",
            "Epoch 00177: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.0567\n",
            "Epoch 178/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 0.9917\n",
            "Epoch 00178: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 0.9918\n",
            "Epoch 179/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0266\n",
            "Epoch 00179: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.0267\n",
            "Epoch 180/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.0385\n",
            "Epoch 00180: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0373\n",
            "Epoch 181/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0358\n",
            "Epoch 00181: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0352\n",
            "Epoch 182/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0390\n",
            "Epoch 00182: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0389\n",
            "Epoch 183/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0247\n",
            "Epoch 00183: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.0248\n",
            "Epoch 184/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.1560\n",
            "Epoch 00184: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.1564\n",
            "Epoch 185/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.0886\n",
            "Epoch 00185: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0879\n",
            "Epoch 186/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0785\n",
            "Epoch 00186: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0786\n",
            "Epoch 187/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0570\n",
            "Epoch 00187: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0570\n",
            "Epoch 188/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0471\n",
            "Epoch 00188: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0471\n",
            "Epoch 189/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0638\n",
            "Epoch 00189: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.0634\n",
            "Epoch 190/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.0460\n",
            "Epoch 00190: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0459\n",
            "Epoch 191/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0130\n",
            "Epoch 00191: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0130\n",
            "Epoch 192/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.0191\n",
            "Epoch 00192: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.0198\n",
            "Epoch 193/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.0407\n",
            "Epoch 00193: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0406\n",
            "Epoch 194/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0422\n",
            "Epoch 00194: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0425\n",
            "Epoch 195/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.0496\n",
            "Epoch 00195: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.0497\n",
            "Epoch 196/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0170\n",
            "Epoch 00196: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0171\n",
            "Epoch 197/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0374\n",
            "Epoch 00197: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0372\n",
            "Epoch 198/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0134\n",
            "Epoch 00198: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0134\n",
            "Epoch 199/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.0222\n",
            "Epoch 00199: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0221\n",
            "Epoch 200/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0285\n",
            "Epoch 00200: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.0285\n",
            "Epoch 201/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0276\n",
            "Epoch 00201: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0276\n",
            "Epoch 202/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0953\n",
            "Epoch 00202: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.0961\n",
            "Epoch 203/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.0212\n",
            "Epoch 00203: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.0222\n",
            "Epoch 204/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.0330\n",
            "Epoch 00204: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0330\n",
            "Epoch 205/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0161\n",
            "Epoch 00205: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0158\n",
            "Epoch 206/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.0406\n",
            "Epoch 00206: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0407\n",
            "Epoch 207/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0071\n",
            "Epoch 00207: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0070\n",
            "Epoch 208/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0010\n",
            "Epoch 00208: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.0010\n",
            "Epoch 209/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0202\n",
            "Epoch 00209: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.0202\n",
            "Epoch 210/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0267\n",
            "Epoch 00210: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0271\n",
            "Epoch 211/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.0049\n",
            "Epoch 00211: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0043\n",
            "Epoch 212/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.0520\n",
            "Epoch 00212: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0524\n",
            "Epoch 213/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0245\n",
            "Epoch 00213: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0246\n",
            "Epoch 214/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0209\n",
            "Epoch 00214: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0210\n",
            "Epoch 215/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.0561\n",
            "Epoch 00215: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0561\n",
            "Epoch 216/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0197\n",
            "Epoch 00216: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0199\n",
            "Epoch 217/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0224\n",
            "Epoch 00217: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0230\n",
            "Epoch 218/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.0458\n",
            "Epoch 00218: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0457\n",
            "Epoch 219/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 0.9955\n",
            "Epoch 00219: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 0.9954\n",
            "Epoch 220/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0037\n",
            "Epoch 00220: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0037\n",
            "Epoch 221/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0422\n",
            "Epoch 00221: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0421\n",
            "Epoch 222/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.0371\n",
            "Epoch 00222: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0383\n",
            "Epoch 223/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.0683\n",
            "Epoch 00223: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0684\n",
            "Epoch 224/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0004\n",
            "Epoch 00224: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 0.9999\n",
            "Epoch 225/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0070\n",
            "Epoch 00225: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0070\n",
            "Epoch 226/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0041\n",
            "Epoch 00226: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0041\n",
            "Epoch 227/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0404\n",
            "Epoch 00227: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0409\n",
            "Epoch 228/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.0547\n",
            "Epoch 00228: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0541\n",
            "Epoch 229/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0618\n",
            "Epoch 00229: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0627\n",
            "Epoch 230/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0469\n",
            "Epoch 00230: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0466\n",
            "Epoch 231/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.0490\n",
            "Epoch 00231: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0490\n",
            "Epoch 232/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0501\n",
            "Epoch 00232: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0505\n",
            "Epoch 233/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0538\n",
            "Epoch 00233: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 6ms/step - loss: 1.0537\n",
            "Epoch 234/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0407\n",
            "Epoch 00234: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0407\n",
            "Epoch 235/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.0656\n",
            "Epoch 00235: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0655\n",
            "Epoch 236/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0395\n",
            "Epoch 00236: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0395\n",
            "Epoch 237/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0860\n",
            "Epoch 00237: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0855\n",
            "Epoch 238/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0703\n",
            "Epoch 00238: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0707\n",
            "Epoch 239/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0837\n",
            "Epoch 00239: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0837\n",
            "Epoch 240/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0532\n",
            "Epoch 00240: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0531\n",
            "Epoch 241/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0426\n",
            "Epoch 00241: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0436\n",
            "Epoch 242/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0441\n",
            "Epoch 00242: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0439\n",
            "Epoch 243/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0551\n",
            "Epoch 00243: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0547\n",
            "Epoch 244/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.0308\n",
            "Epoch 00244: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0302\n",
            "Epoch 245/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.0509\n",
            "Epoch 00245: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0504\n",
            "Epoch 246/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.1097\n",
            "Epoch 00246: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1153\n",
            "Epoch 247/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.9245\n",
            "Epoch 00247: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.9239\n",
            "Epoch 248/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.2767\n",
            "Epoch 00248: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.2767\n",
            "Epoch 249/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.1790\n",
            "Epoch 00249: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1787\n",
            "Epoch 250/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.3146\n",
            "Epoch 00250: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.3147\n",
            "Epoch 251/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.1772\n",
            "Epoch 00251: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1770\n",
            "Epoch 252/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.1659\n",
            "Epoch 00252: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1649\n",
            "Epoch 253/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.2197\n",
            "Epoch 00253: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.2196\n",
            "Epoch 254/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.3456\n",
            "Epoch 00254: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.3476\n",
            "Epoch 255/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.3951\n",
            "Epoch 00255: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.3946\n",
            "Epoch 256/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.1472\n",
            "Epoch 00256: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1476\n",
            "Epoch 257/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.1341\n",
            "Epoch 00257: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1334\n",
            "Epoch 258/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.1699\n",
            "Epoch 00258: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1699\n",
            "Epoch 259/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.1090\n",
            "Epoch 00259: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1090\n",
            "Epoch 260/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.1185\n",
            "Epoch 00260: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1185\n",
            "Epoch 261/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.0940\n",
            "Epoch 00261: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0939\n",
            "Epoch 262/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.0989\n",
            "Epoch 00262: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0991\n",
            "Epoch 263/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0850\n",
            "Epoch 00263: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0847\n",
            "Epoch 264/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0827\n",
            "Epoch 00264: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0848\n",
            "Epoch 265/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0904\n",
            "Epoch 00265: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0904\n",
            "Epoch 266/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0714\n",
            "Epoch 00266: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0714\n",
            "Epoch 267/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0814\n",
            "Epoch 00267: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0815\n",
            "Epoch 268/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0749\n",
            "Epoch 00268: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0762\n",
            "Epoch 269/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0906\n",
            "Epoch 00269: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0906\n",
            "Epoch 270/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0700\n",
            "Epoch 00270: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0709\n",
            "Epoch 271/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0623\n",
            "Epoch 00271: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0625\n",
            "Epoch 272/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0510\n",
            "Epoch 00272: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0513\n",
            "Epoch 273/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0628\n",
            "Epoch 00273: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0624\n",
            "Epoch 274/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0496\n",
            "Epoch 00274: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0498\n",
            "Epoch 275/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0716\n",
            "Epoch 00275: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0711\n",
            "Epoch 276/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.0470\n",
            "Epoch 00276: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0482\n",
            "Epoch 277/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.0699\n",
            "Epoch 00277: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0709\n",
            "Epoch 278/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0333\n",
            "Epoch 00278: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0335\n",
            "Epoch 279/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0497\n",
            "Epoch 00279: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0499\n",
            "Epoch 280/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0594\n",
            "Epoch 00280: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0590\n",
            "Epoch 281/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0469\n",
            "Epoch 00281: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0469\n",
            "Epoch 282/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0113\n",
            "Epoch 00282: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0115\n",
            "Epoch 283/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0325\n",
            "Epoch 00283: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0323\n",
            "Epoch 284/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0446\n",
            "Epoch 00284: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0445\n",
            "Epoch 285/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0310\n",
            "Epoch 00285: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0310\n",
            "Epoch 286/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0314\n",
            "Epoch 00286: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0312\n",
            "Epoch 287/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0429\n",
            "Epoch 00287: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0424\n",
            "Epoch 288/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0717\n",
            "Epoch 00288: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0720\n",
            "Epoch 289/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.0520\n",
            "Epoch 00289: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0521\n",
            "Epoch 290/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.1811\n",
            "Epoch 00290: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1810\n",
            "Epoch 291/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0963\n",
            "Epoch 00291: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0964\n",
            "Epoch 292/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0476\n",
            "Epoch 00292: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0476\n",
            "Epoch 293/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.0933\n",
            "Epoch 00293: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0940\n",
            "Epoch 294/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0470\n",
            "Epoch 00294: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0467\n",
            "Epoch 295/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.0301\n",
            "Epoch 00295: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0302\n",
            "Epoch 296/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0461\n",
            "Epoch 00296: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0458\n",
            "Epoch 297/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0869\n",
            "Epoch 00297: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0862\n",
            "Epoch 298/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.0617\n",
            "Epoch 00298: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0614\n",
            "Epoch 299/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.1143\n",
            "Epoch 00299: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1139\n",
            "Epoch 300/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0747\n",
            "Epoch 00300: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0745\n",
            "Epoch 301/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0690\n",
            "Epoch 00301: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0686\n",
            "Epoch 302/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.0453\n",
            "Epoch 00302: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0450\n",
            "Epoch 303/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0571\n",
            "Epoch 00303: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0572\n",
            "Epoch 304/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0576\n",
            "Epoch 00304: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0576\n",
            "Epoch 305/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0380\n",
            "Epoch 00305: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0372\n",
            "Epoch 306/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.6462\n",
            "Epoch 00306: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.6462\n",
            "Epoch 307/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.7024\n",
            "Epoch 00307: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.7009\n",
            "Epoch 308/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.5245\n",
            "Epoch 00308: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.5241\n",
            "Epoch 309/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.4304\n",
            "Epoch 00309: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.4290\n",
            "Epoch 310/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.3764\n",
            "Epoch 00310: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.3766\n",
            "Epoch 311/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.6155\n",
            "Epoch 00311: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.6149\n",
            "Epoch 312/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.4120\n",
            "Epoch 00312: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.4120\n",
            "Epoch 313/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.2534\n",
            "Epoch 00313: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.2538\n",
            "Epoch 314/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.1384\n",
            "Epoch 00314: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1386\n",
            "Epoch 315/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.1188\n",
            "Epoch 00315: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1194\n",
            "Epoch 316/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.1415\n",
            "Epoch 00316: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1415\n",
            "Epoch 317/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.1098\n",
            "Epoch 00317: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1100\n",
            "Epoch 318/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.1074\n",
            "Epoch 00318: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1074\n",
            "Epoch 319/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0938\n",
            "Epoch 00319: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0936\n",
            "Epoch 320/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0882\n",
            "Epoch 00320: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0877\n",
            "Epoch 321/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.1180\n",
            "Epoch 00321: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1173\n",
            "Epoch 322/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.1010\n",
            "Epoch 00322: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1010\n",
            "Epoch 323/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.0994\n",
            "Epoch 00323: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0992\n",
            "Epoch 324/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.1166\n",
            "Epoch 00324: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1173\n",
            "Epoch 325/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.1067\n",
            "Epoch 00325: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1064\n",
            "Epoch 326/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0633\n",
            "Epoch 00326: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0634\n",
            "Epoch 327/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0701\n",
            "Epoch 00327: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0699\n",
            "Epoch 328/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0636\n",
            "Epoch 00328: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0635\n",
            "Epoch 329/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.1157\n",
            "Epoch 00329: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1163\n",
            "Epoch 330/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0572\n",
            "Epoch 00330: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0568\n",
            "Epoch 331/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.1159\n",
            "Epoch 00331: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1155\n",
            "Epoch 332/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0818\n",
            "Epoch 00332: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0818\n",
            "Epoch 333/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0952\n",
            "Epoch 00333: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1010\n",
            "Epoch 334/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.5473\n",
            "Epoch 00334: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.5468\n",
            "Epoch 335/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.2024\n",
            "Epoch 00335: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.2024\n",
            "Epoch 336/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.1382\n",
            "Epoch 00336: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1382\n",
            "Epoch 337/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.1144\n",
            "Epoch 00337: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1144\n",
            "Epoch 338/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.1207\n",
            "Epoch 00338: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1208\n",
            "Epoch 339/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.1142\n",
            "Epoch 00339: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1142\n",
            "Epoch 340/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.0981\n",
            "Epoch 00340: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0980\n",
            "Epoch 341/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.0846\n",
            "Epoch 00341: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0854\n",
            "Epoch 342/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0814\n",
            "Epoch 00342: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0811\n",
            "Epoch 343/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0648\n",
            "Epoch 00343: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0648\n",
            "Epoch 344/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.1717\n",
            "Epoch 00344: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1726\n",
            "Epoch 345/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.1050\n",
            "Epoch 00345: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1051\n",
            "Epoch 346/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0793\n",
            "Epoch 00346: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0784\n",
            "Epoch 347/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0988\n",
            "Epoch 00347: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0994\n",
            "Epoch 348/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0765\n",
            "Epoch 00348: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0764\n",
            "Epoch 349/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0851\n",
            "Epoch 00349: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0851\n",
            "Epoch 350/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0570\n",
            "Epoch 00350: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0575\n",
            "Epoch 351/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.0689\n",
            "Epoch 00351: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0680\n",
            "Epoch 352/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.0776\n",
            "Epoch 00352: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0782\n",
            "Epoch 353/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0896\n",
            "Epoch 00353: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0899\n",
            "Epoch 354/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0563\n",
            "Epoch 00354: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0564\n",
            "Epoch 355/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.0708\n",
            "Epoch 00355: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.0706\n",
            "Epoch 356/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0834\n",
            "Epoch 00356: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.0836\n",
            "Epoch 357/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0788\n",
            "Epoch 00357: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0791\n",
            "Epoch 358/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.0720\n",
            "Epoch 00358: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0724\n",
            "Epoch 359/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.0758\n",
            "Epoch 00359: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0760\n",
            "Epoch 360/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0769\n",
            "Epoch 00360: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0769\n",
            "Epoch 361/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.2413\n",
            "Epoch 00361: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.2409\n",
            "Epoch 362/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.1033\n",
            "Epoch 00362: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1033\n",
            "Epoch 363/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.0767\n",
            "Epoch 00363: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0768\n",
            "Epoch 364/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0597\n",
            "Epoch 00364: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0597\n",
            "Epoch 365/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.1075\n",
            "Epoch 00365: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1075\n",
            "Epoch 366/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.1003\n",
            "Epoch 00366: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.0998\n",
            "Epoch 367/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0586\n",
            "Epoch 00367: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.0585\n",
            "Epoch 368/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.1180\n",
            "Epoch 00368: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1182\n",
            "Epoch 369/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.2154\n",
            "Epoch 00369: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.2149\n",
            "Epoch 370/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0836\n",
            "Epoch 00370: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0836\n",
            "Epoch 371/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0602\n",
            "Epoch 00371: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0604\n",
            "Epoch 372/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0846\n",
            "Epoch 00372: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0839\n",
            "Epoch 373/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.1551\n",
            "Epoch 00373: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1544\n",
            "Epoch 374/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.1514\n",
            "Epoch 00374: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1511\n",
            "Epoch 375/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.1266\n",
            "Epoch 00375: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1258\n",
            "Epoch 376/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.1106\n",
            "Epoch 00376: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1104\n",
            "Epoch 377/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0859\n",
            "Epoch 00377: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0863\n",
            "Epoch 378/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.0747\n",
            "Epoch 00378: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0754\n",
            "Epoch 379/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0871\n",
            "Epoch 00379: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0871\n",
            "Epoch 380/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.2533\n",
            "Epoch 00380: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.2537\n",
            "Epoch 381/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.3227\n",
            "Epoch 00381: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.3228\n",
            "Epoch 382/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.1477\n",
            "Epoch 00382: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1475\n",
            "Epoch 383/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.2021\n",
            "Epoch 00383: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.2019\n",
            "Epoch 384/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.6365\n",
            "Epoch 00384: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.6358\n",
            "Epoch 385/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.3465\n",
            "Epoch 00385: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.3462\n",
            "Epoch 386/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.1869\n",
            "Epoch 00386: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1863\n",
            "Epoch 387/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.1944\n",
            "Epoch 00387: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1942\n",
            "Epoch 388/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.1861\n",
            "Epoch 00388: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1857\n",
            "Epoch 389/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.3831\n",
            "Epoch 00389: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.3822\n",
            "Epoch 390/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.4490\n",
            "Epoch 00390: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.4493\n",
            "Epoch 391/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.1993\n",
            "Epoch 00391: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1984\n",
            "Epoch 392/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.1625\n",
            "Epoch 00392: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1629\n",
            "Epoch 393/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.1657\n",
            "Epoch 00393: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1650\n",
            "Epoch 394/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.2583\n",
            "Epoch 00394: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.2578\n",
            "Epoch 395/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.1333\n",
            "Epoch 00395: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1333\n",
            "Epoch 396/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.1381\n",
            "Epoch 00396: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1374\n",
            "Epoch 397/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.1039\n",
            "Epoch 00397: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1040\n",
            "Epoch 398/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.1112\n",
            "Epoch 00398: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1114\n",
            "Epoch 399/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.1474\n",
            "Epoch 00399: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1474\n",
            "Epoch 400/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.1230\n",
            "Epoch 00400: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1229\n",
            "Epoch 401/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.1809\n",
            "Epoch 00401: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1809\n",
            "Epoch 402/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.1381\n",
            "Epoch 00402: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1382\n",
            "Epoch 403/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.1276\n",
            "Epoch 00403: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1275\n",
            "Epoch 404/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0905\n",
            "Epoch 00404: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.0905\n",
            "Epoch 405/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.1266\n",
            "Epoch 00405: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1263\n",
            "Epoch 406/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.1109\n",
            "Epoch 00406: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1115\n",
            "Epoch 407/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.0805\n",
            "Epoch 00407: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0805\n",
            "Epoch 408/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.0819\n",
            "Epoch 00408: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.0817\n",
            "Epoch 409/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.0916\n",
            "Epoch 00409: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.0922\n",
            "Epoch 410/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.1093\n",
            "Epoch 00410: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1097\n",
            "Epoch 411/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.1008\n",
            "Epoch 00411: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1010\n",
            "Epoch 412/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.1001\n",
            "Epoch 00412: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.0999\n",
            "Epoch 413/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.0913\n",
            "Epoch 00413: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.0913\n",
            "Epoch 414/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.1378\n",
            "Epoch 00414: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1448\n",
            "Epoch 415/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.2131\n",
            "Epoch 00415: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.2125\n",
            "Epoch 416/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.1393\n",
            "Epoch 00416: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1393\n",
            "Epoch 417/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.1340\n",
            "Epoch 00417: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1343\n",
            "Epoch 418/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.1269\n",
            "Epoch 00418: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1274\n",
            "Epoch 419/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.1244\n",
            "Epoch 00419: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1245\n",
            "Epoch 420/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.1605\n",
            "Epoch 00420: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1606\n",
            "Epoch 421/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.1118\n",
            "Epoch 00421: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1119\n",
            "Epoch 422/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.1383\n",
            "Epoch 00422: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1387\n",
            "Epoch 423/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.1273\n",
            "Epoch 00423: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1267\n",
            "Epoch 424/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.1218\n",
            "Epoch 00424: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1228\n",
            "Epoch 425/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.1427\n",
            "Epoch 00425: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1424\n",
            "Epoch 426/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.1233\n",
            "Epoch 00426: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1245\n",
            "Epoch 427/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.1922\n",
            "Epoch 00427: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1922\n",
            "Epoch 428/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.1415\n",
            "Epoch 00428: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1416\n",
            "Epoch 429/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.1126\n",
            "Epoch 00429: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1126\n",
            "Epoch 430/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.1560\n",
            "Epoch 00430: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 10s 7ms/step - loss: 1.1561\n",
            "Epoch 431/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.2767\n",
            "Epoch 00431: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.2762\n",
            "Epoch 432/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.1469\n",
            "Epoch 00432: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1480\n",
            "Epoch 433/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.0995\n",
            "Epoch 00433: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1003\n",
            "Epoch 434/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.1117\n",
            "Epoch 00434: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1117\n",
            "Epoch 435/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.1093\n",
            "Epoch 00435: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1089\n",
            "Epoch 436/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.1139\n",
            "Epoch 00436: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1145\n",
            "Epoch 437/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.0936\n",
            "Epoch 00437: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.0936\n",
            "Epoch 438/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.1312\n",
            "Epoch 00438: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1318\n",
            "Epoch 439/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.2594\n",
            "Epoch 00439: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.2595\n",
            "Epoch 440/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.1741\n",
            "Epoch 00440: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1736\n",
            "Epoch 441/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.1659\n",
            "Epoch 00441: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1670\n",
            "Epoch 442/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.1223\n",
            "Epoch 00442: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1223\n",
            "Epoch 443/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.1401\n",
            "Epoch 00443: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1401\n",
            "Epoch 444/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.1501\n",
            "Epoch 00444: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1503\n",
            "Epoch 445/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.1627\n",
            "Epoch 00445: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1624\n",
            "Epoch 446/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.1333\n",
            "Epoch 00446: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1326\n",
            "Epoch 447/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.1695\n",
            "Epoch 00447: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1691\n",
            "Epoch 448/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.1673\n",
            "Epoch 00448: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1675\n",
            "Epoch 449/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.1999\n",
            "Epoch 00449: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1999\n",
            "Epoch 450/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.5449\n",
            "Epoch 00450: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.5449\n",
            "Epoch 451/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.8222\n",
            "Epoch 00451: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.8195\n",
            "Epoch 452/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.2249\n",
            "Epoch 00452: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.2246\n",
            "Epoch 453/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.1940\n",
            "Epoch 00453: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1940\n",
            "Epoch 454/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.4231\n",
            "Epoch 00454: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.4222\n",
            "Epoch 455/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.3203\n",
            "Epoch 00455: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.3200\n",
            "Epoch 456/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.2143\n",
            "Epoch 00456: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.2144\n",
            "Epoch 457/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.1863\n",
            "Epoch 00457: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.1859\n",
            "Epoch 458/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 2.7452\n",
            "Epoch 00458: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 2.7444\n",
            "Epoch 459/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.8198\n",
            "Epoch 00459: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.8187\n",
            "Epoch 460/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.5842\n",
            "Epoch 00460: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.5836\n",
            "Epoch 461/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.5416\n",
            "Epoch 00461: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.5412\n",
            "Epoch 462/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.5005\n",
            "Epoch 00462: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.4997\n",
            "Epoch 463/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 2.0930\n",
            "Epoch 00463: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 2.0949\n",
            "Epoch 464/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 2.0673\n",
            "Epoch 00464: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 2.0652\n",
            "Epoch 465/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.7822\n",
            "Epoch 00465: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.7804\n",
            "Epoch 466/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.4997\n",
            "Epoch 00466: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.4992\n",
            "Epoch 467/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.3481\n",
            "Epoch 00467: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.3478\n",
            "Epoch 468/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.2953\n",
            "Epoch 00468: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.2953\n",
            "Epoch 469/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.2995\n",
            "Epoch 00469: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.2995\n",
            "Epoch 470/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.2656\n",
            "Epoch 00470: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.2655\n",
            "Epoch 471/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.2459\n",
            "Epoch 00471: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.2463\n",
            "Epoch 472/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.8447\n",
            "Epoch 00472: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.8463\n",
            "Epoch 473/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 2.1297\n",
            "Epoch 00473: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 2.1297\n",
            "Epoch 474/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.8373\n",
            "Epoch 00474: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.8381\n",
            "Epoch 475/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.8666\n",
            "Epoch 00475: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.8666\n",
            "Epoch 476/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.7839\n",
            "Epoch 00476: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.7832\n",
            "Epoch 477/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 2.0961\n",
            "Epoch 00477: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 2.0952\n",
            "Epoch 478/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.6499\n",
            "Epoch 00478: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.6497\n",
            "Epoch 479/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.8714\n",
            "Epoch 00479: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.8707\n",
            "Epoch 480/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.5944\n",
            "Epoch 00480: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.5935\n",
            "Epoch 481/500\n",
            "1495/1500 [============================>.] - ETA: 0s - loss: 1.5598\n",
            "Epoch 00481: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.5590\n",
            "Epoch 482/500\n",
            "1496/1500 [============================>.] - ETA: 0s - loss: 1.5250\n",
            "Epoch 00482: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.5249\n",
            "Epoch 483/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.6871\n",
            "Epoch 00483: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.6862\n",
            "Epoch 484/500\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.5337\n",
            "Epoch 00484: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.5334\n",
            "Epoch 485/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.5192\n",
            "Epoch 00485: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.5192\n",
            "Epoch 486/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.5441\n",
            "Epoch 00486: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.5436\n",
            "Epoch 487/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.4703\n",
            "Epoch 00487: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.4703\n",
            "Epoch 488/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.4491\n",
            "Epoch 00488: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.4519\n",
            "Epoch 489/500\n",
            "1494/1500 [============================>.] - ETA: 0s - loss: 1.6979\n",
            "Epoch 00489: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.6962\n",
            "Epoch 490/500\n",
            "1497/1500 [============================>.] - ETA: 0s - loss: 1.6174\n",
            "Epoch 00490: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.6170\n",
            "Epoch 491/500\n",
            "1493/1500 [============================>.] - ETA: 0s - loss: 1.4378\n",
            "Epoch 00491: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.4374\n",
            "Epoch 492/500\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 1.4982\n",
            "Epoch 00492: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.4982\n",
            "Epoch 493/500\n",
            "1499/1500 [============================>.] - ETA: 0s - loss: 1.5456\n",
            "Epoch 00493: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 11s 7ms/step - loss: 1.5459\n",
            "Epoch 494/500\n",
            " 494/1500 [========>.....................] - ETA: 7s - loss: 1.5236WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 750000 batches). You may need to use the repeat() function when building your dataset.\n",
            "\n",
            "Epoch 00494: loss did not improve from 0.97047\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 1.5246\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4967650510>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4-2.\n",
        "Loading model**"
      ],
      "metadata": {
        "id": "iSTJDFos7Ego"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# restart from checkpoint\n",
        "model = create_model(max_sequence_len, total_words)\n",
        "\n",
        "model.load_weights(checkpoint_path)\n"
      ],
      "metadata": {
        "id": "4ae4oMgu7Dkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Generating the text**\n",
        "\n",
        "Great, our model architecture is now ready and we can train it using our data. Next lets write the function to predict the next word based on the input words (or seed text). We will first tokenize the seed text, pad the sequences and pass into the trained model to get predicted word. The multiple predicted words can be appended together to get predicted sequence."
      ],
      "metadata": {
        "id": "j75m_NTjKBqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\n",
        "\n",
        "        predict_x=model.predict(token_list)\n",
        "        predicted=np.argmax(predict_x,axis=1)\n",
        "\n",
        "\n",
        "        output_word = \"\"\n",
        "        for word,index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \"+output_word\n",
        "    return seed_text.title()"
      ],
      "metadata": {
        "id": "Sjl0M_a8KHCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Some Results**"
      ],
      "metadata": {
        "id": "b896hjiGKRt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print (generate_text(\"air\", 12, model, max_sequence_len))\n",
        "print (generate_text(\"trump\", 14, model, max_sequence_len))\n",
        "print (generate_text(\"donald trump\", 14, model, max_sequence_len))\n",
        "print (generate_text(\"india\", 18, model, max_sequence_len))\n",
        "print (generate_text(\"new york\", 14, model, max_sequence_len))\n",
        "print (generate_text(\"science and technology\", 11, model, max_sequence_len))"
      ],
      "metadata": {
        "id": "Geri1WulKX2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Improvement Ideas\n",
        "As we can see, the model has produced the output which looks fairly fine. The results can be improved further with following points:\n",
        "\n",
        "Adding more data\n",
        "Fine Tuning the network architecture\n",
        "Fine Tuning the network parameters\n",
        "Thanks for going through the notebook, please upvote if you liked."
      ],
      "metadata": {
        "id": "HMyvxJa6K8a4"
      }
    }
  ]
}