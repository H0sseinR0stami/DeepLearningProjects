{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/H0sseinR0stami/DeepLearningProjects/blob/main/Text_generation/text_generation_with_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "# Text generation with an RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:13:15.793068Z",
          "iopub.status.busy": "2022-01-26T01:13:15.792534Z",
          "iopub.status.idle": "2022-01-26T01:13:17.714563Z",
          "shell.execute_reply": "2022-01-26T01:13:17.713959Z"
        },
        "id": "yG_n40gFzf9s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "import keras\n",
        "# set seeds for reproducability\n",
        "from numpy.random import seed\n",
        "tf.random.set_seed(2)\n",
        "seed(1)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, os\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:13:17.719393Z",
          "iopub.status.busy": "2022-01-26T01:13:17.718756Z",
          "iopub.status.idle": "2022-01-26T01:13:17.818912Z",
          "shell.execute_reply": "2022-01-26T01:13:17.818481Z"
        },
        "id": "pD_55cOxLkAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "254026a9-42eb-480f-b397-2940ac66c330"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "curr_dir = '/content/drive/MyDrive/Colab_Notebooks/project5/database/'\n",
        "all_headlines = []\n",
        "for filename in os.listdir(curr_dir):\n",
        "  if 'CommentsJan2017' in filename:\n",
        "      print(filename)\n",
        "      article_df = pd.read_csv(curr_dir + filename)\n",
        "      all_headlines.extend(list(article_df.commentBody.values))\n",
        "       ##df = pd.read_csv('matrix.txt', sep=',', header=None, skiprows=1000, nrows=1000)\n",
        "\n",
        "all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
        "len(all_headlines)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXEMmBQYJ1iv",
        "outputId": "46134c74-6f81-4864-f0e0-5095d0823290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CommentsJan2017.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "231449"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### Read the data\n",
        "\n",
        "Gathering data from .csv file and save it in the text.txt:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(txt):\n",
        "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
        "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
        "    return txt\n",
        "\n",
        "text_list = [clean_text(x) for x in all_headlines]\n",
        "text = \" \".join(str(y) for y in text_list)\n",
        "\n",
        "path_to_file = \"/content/drive/MyDrive/Colab_Notebooks/project5/text.txt\"\n",
        "text_file = open( path_to_file , \"w\")\n",
        "text_file.write(text)\n",
        "text_file.close()"
      ],
      "metadata": {
        "id": "fX1YMAafK1SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:13:17.830382Z",
          "iopub.status.busy": "2022-01-26T01:13:17.829824Z",
          "iopub.status.idle": "2022-01-26T01:13:17.832088Z",
          "shell.execute_reply": "2022-01-26T01:13:17.832414Z"
        },
        "id": "Duhg9NrUymwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0d3aef5-6891-402f-c83a-f948afba1e67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 98701284 characters\n",
            "for all you americans out there  still rejoicing over the majority win of  republicans over the legislature of this landbrbewarebrjust like you would have been if  there were any other kind of majoritybrthe founding fathers had something like this in\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "path_to_file = \"/content/drive/MyDrive/Colab_Notebooks/project5/text.txt\"\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')\n",
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:13:17.849024Z",
          "iopub.status.busy": "2022-01-26T01:13:17.848368Z",
          "iopub.status.idle": "2022-01-26T01:13:17.850850Z",
          "shell.execute_reply": "2022-01-26T01:13:17.851214Z"
        },
        "id": "IlCgQBRVymwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2505aee3-db39-4274-e9fe-6d2824c6beba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39 unique characters\n",
            "['\\t', '\\r', ' ', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "L5GvxOyw6cz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "mLG47vHP6ihW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "mTQSTdDC6t03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:13:19.500691Z",
          "iopub.status.busy": "2022-01-26T01:13:19.500106Z",
          "iopub.status.idle": "2022-01-26T01:13:19.900309Z",
          "shell.execute_reply": "2022-01-26T01:13:19.899715Z"
        },
        "id": "UopbsKi88tm5"
      },
      "outputs": [],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:13:19.904729Z",
          "iopub.status.busy": "2022-01-26T01:13:19.904087Z",
          "iopub.status.idle": "2022-01-26T01:13:19.906024Z",
          "shell.execute_reply": "2022-01-26T01:13:19.906380Z"
        },
        "id": "qmxrYDCTy-eL"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:13:19.929943Z",
          "iopub.status.busy": "2022-01-26T01:13:19.929398Z",
          "iopub.status.idle": "2022-01-26T01:13:19.931042Z",
          "shell.execute_reply": "2022-01-26T01:13:19.931384Z"
        },
        "id": "C-G2oaTxy6km"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:13:19.935769Z",
          "iopub.status.busy": "2022-01-26T01:13:19.935221Z",
          "iopub.status.idle": "2022-01-26T01:13:19.944293Z",
          "shell.execute_reply": "2022-01-26T01:13:19.944756Z"
        },
        "id": "BpdjRO2CzOfZ"
      },
      "outputs": [],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:13:19.962766Z",
          "iopub.status.busy": "2022-01-26T01:13:19.962242Z",
          "iopub.status.idle": "2022-01-26T01:13:19.964148Z",
          "shell.execute_reply": "2022-01-26T01:13:19.964540Z"
        },
        "id": "9NGu-FkO_kYU"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:13:19.991544Z",
          "iopub.status.busy": "2022-01-26T01:13:19.975220Z",
          "iopub.status.idle": "2022-01-26T01:13:20.017034Z",
          "shell.execute_reply": "2022-01-26T01:13:20.017414Z"
        },
        "id": "B9iKPXkw5xwa"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### Create training batches\n",
        "\n",
        "You used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, you need to shuffle the data and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:13:20.045787Z",
          "iopub.status.busy": "2022-01-26T01:13:20.043665Z",
          "iopub.status.idle": "2022-01-26T01:13:20.050603Z",
          "shell.execute_reply": "2022-01-26T01:13:20.050999Z"
        },
        "id": "p2pGotuNzf-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2aeb331-d3f9-4c3f-e8cf-360bcefaf477"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<RepeatDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    .repeat(count=5))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "This section defines the model as a `keras.Model` subclass (For details see [Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)).\n",
        "\n",
        "This model has three layers:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map each character-ID to a vector with `embedding_dim` dimensions;\n",
        "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use an LSTM layer here.)\n",
        "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:13:20.055210Z",
          "iopub.status.busy": "2022-01-26T01:13:20.054590Z",
          "iopub.status.idle": "2022-01-26T01:13:20.056771Z",
          "shell.execute_reply": "2022-01-26T01:13:20.056369Z"
        },
        "id": "zHT8cLh7EAsg"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:13:20.063080Z",
          "iopub.status.busy": "2022-01-26T01:13:20.062462Z",
          "iopub.status.idle": "2022-01-26T01:13:20.064186Z",
          "shell.execute_reply": "2022-01-26T01:13:20.064604Z"
        },
        "id": "wj8HQ2w8z4iO"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "class RNN(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, dim, num_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.num_layers = num_layers\n",
        "        def layer():\n",
        "            return tf.keras.layers.GRU(\n",
        "                self.dim,\n",
        "                return_sequences=True,\n",
        "                return_state=True,\n",
        "                stateful=True)\n",
        "        self._layer_names = ['layer_' + str(i) for i in range(self.num_layers)]\n",
        "        for name in self._layer_names:\n",
        "             self.__setattr__(name, layer())\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seqs = inputs\n",
        "        state = None\n",
        "        for name in self._layer_names:\n",
        "            rnn = self.__getattribute__(name)\n",
        "            (seqs, state) = rnn(seqs, initial_state=state)\n",
        "        return seqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:13:20.068373Z",
          "iopub.status.busy": "2022-01-26T01:13:20.067845Z",
          "iopub.status.idle": "2022-01-26T01:13:20.080121Z",
          "shell.execute_reply": "2022-01-26T01:13:20.079718Z"
        },
        "id": "IX58Xj9z47Aw"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:13:20.084163Z",
          "iopub.status.busy": "2022-01-26T01:13:20.083540Z",
          "iopub.status.idle": "2022-01-26T01:13:25.996465Z",
          "shell.execute_reply": "2022-01-26T01:13:25.996020Z"
        },
        "id": "C-_70kKAPrPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0e863a0-acb7-4d6a-db90-c34728cc2ad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 40) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6NzLBi4VM4o"
      },
      "source": [
        "In the above example the sequence length of the input is `100` but the model can be run on inputs of any length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:13:26.000658Z",
          "iopub.status.busy": "2022-01-26T01:13:26.000125Z",
          "iopub.status.idle": "2022-01-26T01:13:26.011459Z",
          "shell.execute_reply": "2022-01-26T01:13:26.010336Z"
        },
        "id": "vPGmAAXmVLGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cba7453f-29d0-4b19-901b-ccabf50b4468"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  10240     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  41000     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,989,544\n",
            "Trainable params: 3,989,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwv0gEkURfx1"
      },
      "source": [
        "To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "\n",
        "Note: It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
        "\n",
        "Try it for the first example in the batch:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM1Vbxs_URw5"
      },
      "source": [
        "This gives us, at each timestep, a prediction of the next character index:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCbHQHiaa4Ic"
      },
      "source": [
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "### Attach an optimizer, and a loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAjbjY03eiQ4"
      },
      "source": [
        "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because your model returns logits, you need to set the `from_logits` flag.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:13:26.037241Z",
          "iopub.status.busy": "2022-01-26T01:13:26.036657Z",
          "iopub.status.idle": "2022-01-26T01:13:26.038892Z",
          "shell.execute_reply": "2022-01-26T01:13:26.038343Z"
        },
        "id": "ZOeWdgxNFDXq"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:13:26.042710Z",
          "iopub.status.busy": "2022-01-26T01:13:26.042183Z",
          "iopub.status.idle": "2022-01-26T01:13:26.047921Z",
          "shell.execute_reply": "2022-01-26T01:13:26.047514Z"
        },
        "id": "4HrXTACTdzY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4eba274-fd42-4d62-b812-440b72aa368d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 40)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(3.6875403, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkvUIneTFiow"
      },
      "source": [
        "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:13:26.051317Z",
          "iopub.status.busy": "2022-01-26T01:13:26.050756Z",
          "iopub.status.idle": "2022-01-26T01:13:26.054467Z",
          "shell.execute_reply": "2022-01-26T01:13:26.053949Z"
        },
        "id": "MAJfS5YoFiHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "278d2b6a-0472-4db1-836f-013a67be211e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39.94647"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeOXriLcymww"
      },
      "source": [
        "Configure the training procedure using the `tf.keras.Model.compile` method. Use `tf.keras.optimizers.Adam` with default arguments and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:13:26.061178Z",
          "iopub.status.busy": "2022-01-26T01:13:26.060644Z",
          "iopub.status.idle": "2022-01-26T01:13:26.065560Z",
          "shell.execute_reply": "2022-01-26T01:13:26.065888Z"
        },
        "id": "DDl1_Een6rL0"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### Configure checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6XBUUavgF56"
      },
      "source": [
        "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:13:26.070010Z",
          "iopub.status.busy": "2022-01-26T01:13:26.069463Z",
          "iopub.status.idle": "2022-01-26T01:13:26.071465Z",
          "shell.execute_reply": "2022-01-26T01:13:26.071047Z"
        },
        "id": "W6fWTriUZP-n"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Include the epoch in the file name (uses `str.format`)\n",
        "checkpoint_path = \"/content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "batch_size = 64\n",
        "    #save_freq=640*batch_size\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    verbose=1,\n",
        "    save_freq=80*batch_size,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### Execute the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxdOA-rgyGvs"
      },
      "source": [
        "To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:13:26.075075Z",
          "iopub.status.busy": "2022-01-26T01:13:26.074386Z",
          "iopub.status.idle": "2022-01-26T01:13:26.076168Z",
          "shell.execute_reply": "2022-01-26T01:13:26.076494Z"
        },
        "id": "7yGBE2zxMMHs"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:13:26.080539Z",
          "iopub.status.busy": "2022-01-26T01:13:26.079959Z",
          "iopub.status.idle": "2022-01-26T01:15:19.134183Z",
          "shell.execute_reply": "2022-01-26T01:15:19.134581Z"
        },
        "id": "UK-hmKjYVoll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fa0fcce-3627-4818-dd33-e0b40ce6cbb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "256/256 [==============================] - 38s 135ms/step - loss: 1.6091\n",
            "Epoch 2/100\n",
            "256/256 [==============================] - 34s 134ms/step - loss: 1.4279\n",
            "Epoch 3/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.3634\n",
            "Epoch 4/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.3190\n",
            "Epoch 5/100\n",
            "256/256 [==============================] - 34s 134ms/step - loss: 1.2898\n",
            "Epoch 6/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.2418\n",
            "Epoch 7/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.2268\n",
            "Epoch 8/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.2162\n",
            "Epoch 9/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.2159\n",
            "Epoch 10/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.2023\n",
            "Epoch 11/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1964\n",
            "Epoch 12/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1918\n",
            "Epoch 13/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1698\n",
            "Epoch 14/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1583\n",
            "Epoch 15/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1677\n",
            "Epoch 16/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.2203\n",
            "Epoch 17/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.2157\n",
            "Epoch 18/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1880\n",
            "Epoch 19/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1809\n",
            "Epoch 20/100\n",
            "255/256 [============================>.] - ETA: 0s - loss: 1.1680\n",
            "Epoch 00020: saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/cp-0020.ckpt\n",
            "256/256 [==============================] - 34s 134ms/step - loss: 1.1680\n",
            "Epoch 21/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1826\n",
            "Epoch 22/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1699\n",
            "Epoch 23/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1588\n",
            "Epoch 24/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1720\n",
            "Epoch 25/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1786\n",
            "Epoch 26/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1684\n",
            "Epoch 27/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1554\n",
            "Epoch 28/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1418\n",
            "Epoch 29/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1557\n",
            "Epoch 30/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1392\n",
            "Epoch 31/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1389\n",
            "Epoch 32/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1754\n",
            "Epoch 33/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.2410\n",
            "Epoch 34/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1585\n",
            "Epoch 35/100\n",
            "256/256 [==============================] - 34s 131ms/step - loss: 1.1531\n",
            "Epoch 36/100\n",
            "256/256 [==============================] - 34s 131ms/step - loss: 1.1690\n",
            "Epoch 37/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1664\n",
            "Epoch 38/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1408\n",
            "Epoch 39/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1507\n",
            "Epoch 40/100\n",
            "255/256 [============================>.] - ETA: 0s - loss: 1.1523\n",
            "Epoch 00040: saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/cp-0040.ckpt\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1520\n",
            "Epoch 41/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1285\n",
            "Epoch 42/100\n",
            "256/256 [==============================] - 34s 131ms/step - loss: 1.1323\n",
            "Epoch 43/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1296\n",
            "Epoch 44/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1334\n",
            "Epoch 45/100\n",
            "256/256 [==============================] - 34s 131ms/step - loss: 1.1258\n",
            "Epoch 46/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1277\n",
            "Epoch 47/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1235\n",
            "Epoch 48/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1285\n",
            "Epoch 49/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1433\n",
            "Epoch 50/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1624\n",
            "Epoch 51/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1890\n",
            "Epoch 52/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1770\n",
            "Epoch 53/100\n",
            "256/256 [==============================] - 34s 131ms/step - loss: 1.1785\n",
            "Epoch 54/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1849\n",
            "Epoch 55/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1830\n",
            "Epoch 56/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1658\n",
            "Epoch 57/100\n",
            "256/256 [==============================] - 34s 131ms/step - loss: 1.1755\n",
            "Epoch 58/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1636\n",
            "Epoch 59/100\n",
            "256/256 [==============================] - 34s 131ms/step - loss: 1.1587\n",
            "Epoch 60/100\n",
            "255/256 [============================>.] - ETA: 0s - loss: 1.1598\n",
            "Epoch 00060: saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/cp-0060.ckpt\n",
            "256/256 [==============================] - 35s 138ms/step - loss: 1.1596\n",
            "Epoch 61/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1477\n",
            "Epoch 62/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1559\n",
            "Epoch 63/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1594\n",
            "Epoch 64/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1588\n",
            "Epoch 65/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1412\n",
            "Epoch 66/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1295\n",
            "Epoch 67/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1225\n",
            "Epoch 68/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1366\n",
            "Epoch 69/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1364\n",
            "Epoch 70/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1268\n",
            "Epoch 71/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1398\n",
            "Epoch 72/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1257\n",
            "Epoch 73/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1171\n",
            "Epoch 74/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1084\n",
            "Epoch 75/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1388\n",
            "Epoch 76/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.2140\n",
            "Epoch 77/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1651\n",
            "Epoch 78/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1744\n",
            "Epoch 79/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1517\n",
            "Epoch 80/100\n",
            "255/256 [============================>.] - ETA: 0s - loss: 1.1519\n",
            "Epoch 00080: saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/cp-0080.ckpt\n",
            "256/256 [==============================] - 34s 134ms/step - loss: 1.1519\n",
            "Epoch 81/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1715\n",
            "Epoch 82/100\n",
            "256/256 [==============================] - 34s 133ms/step - loss: 1.1397\n",
            "Epoch 83/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1423\n",
            "Epoch 84/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1711\n",
            "Epoch 85/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.1642\n",
            "Epoch 86/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.2095\n",
            "Epoch 87/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.2131\n",
            "Epoch 88/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.3650\n",
            "Epoch 89/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.5933\n",
            "Epoch 90/100\n",
            "256/256 [==============================] - 34s 134ms/step - loss: 1.6657\n",
            "Epoch 91/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.5823\n",
            "Epoch 92/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.6516\n",
            "Epoch 93/100\n",
            "256/256 [==============================] - 34s 131ms/step - loss: 1.6557\n",
            "Epoch 94/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.5837\n",
            "Epoch 95/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.6014\n",
            "Epoch 96/100\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.6216\n",
            "Epoch 97/100\n",
            "256/256 [==============================] - 34s 131ms/step - loss: 1.5768\n",
            "Epoch 98/100\n",
            "256/256 [==============================] - 33s 131ms/step - loss: 1.5315\n",
            "Epoch 99/100\n",
            "256/256 [==============================] - 34s 131ms/step - loss: 1.5275\n",
            "Epoch 100/100\n",
            "255/256 [============================>.] - ETA: 0s - loss: 1.4951\n",
            "Epoch 00100: saving model to /content/drive/MyDrive/Colab_Notebooks/project5/training_checkpoints/cp-0100.ckpt\n",
            "256/256 [==============================] - 34s 132ms/step - loss: 1.4949\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset,steps_per_epoch=256, epochs=EPOCHS, callbacks=[cp_callback])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0FHQfdgquV4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# restart from checkpoint\n",
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "model.load_weights(latest)"
      ],
      "metadata": {
        "id": "zhvvA5jv9aFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIdQ8c8NvMzV"
      },
      "source": [
        "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as you execute it.\n",
        "\n",
        "![To generate text the model's output is fed back to the input](images/text_generation_sampling.png)\n",
        "\n",
        "Each time you call the model you pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "The following makes a single step prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:15:19.144258Z",
          "iopub.status.busy": "2022-01-26T01:15:19.143678Z",
          "iopub.status.idle": "2022-01-26T01:15:19.145395Z",
          "shell.execute_reply": "2022-01-26T01:15:19.145741Z"
        },
        "id": "iSBU1tHmlUSs"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:15:19.149948Z",
          "iopub.status.busy": "2022-01-26T01:15:19.149410Z",
          "iopub.status.idle": "2022-01-26T01:15:19.156740Z",
          "shell.execute_reply": "2022-01-26T01:15:19.156267Z"
        },
        "id": "fqMOuDutnOxK"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9yDoa0G3IgQ"
      },
      "source": [
        "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:15:19.162758Z",
          "iopub.status.busy": "2022-01-26T01:15:19.162150Z",
          "iopub.status.idle": "2022-01-26T01:15:21.837802Z",
          "shell.execute_reply": "2022-01-26T01:15:21.837327Z"
        },
        "id": "ST7PSyk9t1mT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "931ceae8-f8c3-47fd-e052-7d021a1ae7e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "air meas the wallsbrhot need you learn that a beaute why and living searest your likes a jusks voter gr \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.43666672706604\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['air'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM2Uma_-yVIq"
      },
      "source": [
        "The easiest thing you can do to improve the results is to train it for longer (try `EPOCHS = 30`).\n",
        "\n",
        "You can also experiment with a different start string, try adding another RNN layer to improve the model's accuracy, or adjust the temperature parameter to generate more or less random predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OfbI4aULmuj"
      },
      "source": [
        "If you want the model to generate text *faster* the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:15:21.844317Z",
          "iopub.status.busy": "2022-01-26T01:15:21.843483Z",
          "iopub.status.idle": "2022-01-26T01:15:24.347791Z",
          "shell.execute_reply": "2022-01-26T01:15:24.347240Z"
        },
        "id": "ZkLu7Y8UCMT7"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['trump', 'weather', 'ball:', 'water:', 'air'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlUQzwu6EXam"
      },
      "source": [
        "## Export the generator\n",
        "\n",
        "This single-step model can easily be [saved and restored](https://www.tensorflow.org/guide/saved_model), allowing you to use it anywhere a `tf.saved_model` is accepted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:15:24.352662Z",
          "iopub.status.busy": "2022-01-26T01:15:24.351980Z",
          "iopub.status.idle": "2022-01-26T01:15:29.658526Z",
          "shell.execute_reply": "2022-01-26T01:15:29.657900Z"
        },
        "id": "3Grk32H_CzsC"
      },
      "outputs": [],
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4QwTjAM6A2O"
      },
      "source": [
        "## Advanced: Customized Training\n",
        "\n",
        "The above training procedure is simple, but does not give you much control.\n",
        "It uses teacher-forcing which prevents bad predictions from being fed back to the model, so the model never learns to recover from mistakes.\n",
        "\n",
        "So now that you've seen how to run the model manually next you'll implement the training loop. This gives a starting point if, for example, you want to implement _curriculum  learning_ to help stabilize the model's open-loop output.\n",
        "\n",
        "The most important part of a custom training loop is the train step function.\n",
        "\n",
        "Use `tf.GradientTape` to track the gradients. You can learn more about this approach by reading the [eager execution guide](https://www.tensorflow.org/guide/eager).\n",
        "\n",
        "The basic procedure is:\n",
        "\n",
        "1. Execute the model and calculate the loss under a `tf.GradientTape`.\n",
        "2. Calculate the updates and apply them to the model using the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:15:30.194452Z",
          "iopub.status.busy": "2022-01-26T01:15:30.193898Z",
          "iopub.status.idle": "2022-01-26T01:15:30.195918Z",
          "shell.execute_reply": "2022-01-26T01:15:30.195527Z"
        },
        "id": "x0pZ101hjwW0"
      },
      "outputs": [],
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Oc-eJALcK8B"
      },
      "source": [
        "The above implementation of the `train_step` method follows [Keras' `train_step` conventions](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit). This is optional, but it allows you to change the behavior of the train step and still use keras' `Model.compile` and `Model.fit` methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:15:30.199707Z",
          "iopub.status.busy": "2022-01-26T01:15:30.199130Z",
          "iopub.status.idle": "2022-01-26T01:15:30.207044Z",
          "shell.execute_reply": "2022-01-26T01:15:30.206499Z"
        },
        "id": "XKyWiZ_Lj7w5"
      },
      "outputs": [],
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:15:30.212694Z",
          "iopub.status.busy": "2022-01-26T01:15:30.211886Z",
          "iopub.status.idle": "2022-01-26T01:15:30.216168Z",
          "shell.execute_reply": "2022-01-26T01:15:30.215716Z"
        },
        "id": "U817KUm7knlm"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:15:30.219498Z",
          "iopub.status.busy": "2022-01-26T01:15:30.218298Z",
          "iopub.status.idle": "2022-01-26T01:15:37.619686Z",
          "shell.execute_reply": "2022-01-26T01:15:37.619163Z"
        },
        "id": "o694aoBPnEi9"
      },
      "outputs": [],
      "source": [
        "model.fit(dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8nAtKHVoInR"
      },
      "source": [
        "Or if you need more control, you can write your own complete custom training loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-26T01:15:37.628930Z",
          "iopub.status.busy": "2022-01-26T01:15:37.628288Z",
          "iopub.status.idle": "2022-01-26T01:16:31.497075Z",
          "shell.execute_reply": "2022-01-26T01:16:31.496303Z"
        },
        "id": "d4tSNwymzf-q"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['weather:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "id": "IiJo1ZKoNRCG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}